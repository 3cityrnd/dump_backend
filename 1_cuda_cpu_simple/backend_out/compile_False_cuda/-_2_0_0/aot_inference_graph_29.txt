class <lambda>(torch.nn.Module):
    def forward(self, arg0_1: "f32[1, 4][4, 1]cuda:0"):
         # File: /home/pablo/torch_compile/dump_backend.py:25 in torch_dynamo_resume_in_forward_at_24, code: x = x+50.0
        add: "f32[1, 4][4, 1]cuda:0" = torch.ops.aten.add.Tensor(arg0_1, 50.0);  arg0_1 = None
        return (add,)
        